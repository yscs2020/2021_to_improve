{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc07b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def kappa(confusion_matrix):\n",
    "    N = np.sum(confusion_matrix)\n",
    "    sum_po = 0\n",
    "    sum_pe = 0\n",
    "    for i in range(len(confusion_matrix)):\n",
    "        sum_po += confusion_matrix[i][i]\n",
    "        i_row = np.sum(confusion_matrix[i,:])\n",
    "        i_col = np.sum(confusion_matrix[:,i])\n",
    "        sum_pe += i_col * i_row\n",
    "    po = sum_po/N\n",
    "    pe = sum_pe/(N*N)\n",
    "    kia = (po-pe)/(1-pe)\n",
    "    return kia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f451169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loadDataSet():\n",
    "    dataSet = [['my','dog','has','flea','problems','help','please'],\n",
    "                  ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                  ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                  ['stop','posting','stupid','worthless','garbage'],\n",
    "                  ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                  ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return dataSet,classVec\n",
    "def createVocabList(dataSet):\n",
    "    vacabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vacabSet = vacabSet | set(document)\n",
    "    vacabList = list(vacabSet)\n",
    "    return vacabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]=1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29cdfb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(trainMatrix,classVec):\n",
    "    num_docs = len(trainMatrix)\n",
    "    num_words = len(trainMatrix[0])\n",
    "    pAb = sum(classVec)/float(num_docs)\n",
    "    p0Num = np.zeros(num_words)\n",
    "    p0Denom = 0.0\n",
    "    p1Num = np.zeros(num_words)\n",
    "    p1Denom = 0.0\n",
    "    for i in range(num_docs):\n",
    "        if classVec[i]==1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p0V = p0Num/p0Denom\n",
    "    p1V = p1Num/p1Denom\n",
    "    return p0V,p1V,pAb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5256e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify,p0V,p1V,pAb):\n",
    "    from functools import reduce\n",
    "    p1 = reduce(lambda x,y:x*y,vec2Classify*p1V)*pAb\n",
    "    p2 = reduce(lambda x,y:x*y,vec2Classify*p0V)*(1-pAb)\n",
    "    if p1>p2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ad6fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB(testVec):\n",
    "    dataSet,classVec = loadDataSet()\n",
    "    vocabList = createVocabList(dataSet)\n",
    "    trainMat = []\n",
    "    for inputSet in dataSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList,inputSet))\n",
    "    p0V,p1V,pAb= trainNB(np.array(trainMat),np.array(classVec))\n",
    "    thisDoc = np.array(setOfWords2Vec(vocabList,testVec))\n",
    "    if classifyNB(thisDoc,p0V,p1V,pAb):\n",
    "        print('侮辱')\n",
    "    else:\n",
    "        print('非侮辱')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bab835ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非侮辱\n",
      "非侮辱\n"
     ]
    }
   ],
   "source": [
    "testEntry = ['love', 'my', 'dalmation']\n",
    "testingNB(testEntry)\n",
    "# 测试文本2\n",
    "testEntry = ['stupid', 'garbage']\n",
    "testingNB(testEntry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b2461bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(trainMatrix,classVec):\n",
    "    num_docs = len(trainMatrix)\n",
    "    num_words = len(trainMatrix[0])\n",
    "    pAb = sum(classVec)/float(num_docs)\n",
    "    p0Num = np.ones(num_words)\n",
    "    p0Denom = 2.0\n",
    "    p1Num = np.ones(num_words)\n",
    "    p1Denom = 2.0\n",
    "    for i in range(num_docs):\n",
    "        if classVec[i]==1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p0V = np.log(p0Num/p0Denom)\n",
    "    p1V = np.log(p1Num/p1Denom)\n",
    "    return p0V,p1V,pAb\n",
    "def classifyNB(vec2Classify,p0V,p1V,pAb):\n",
    "    p1 = sum(vec2Classify*p1V)+np.log(pAb)\n",
    "    p0 = sum(vec2Classify*p0v)+np.log(1-pAb)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4c8f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*',bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) >2]\n",
    "def spanTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('email/spam/%d.txt'% i).read())\n",
    "        docList.append(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt'% i).read())\n",
    "        docList.append(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainSet = list(np.arange(50))\n",
    "    testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0,len(trainSet)))\n",
    "        testSet.append(trainSet[randIndex])\n",
    "        del trainSet[randIndex]\n",
    "    trainMat = []\n",
    "    trainClass = []\n",
    "    for docIndex in trainSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))\n",
    "        trainClass.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB(np.array(trainMat),np.array(trainClass))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVect = setOfWords2Vec(vocabList,docList[docIndex])\n",
    "        if classifyNB(wordVect,p0V,p1V,pSpam)!=classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print('classification error',docList[docIndex])\n",
    "    print('the error rate is :',float(errorCount)/len(testSet))\n",
    "    return float(errorCount)/len(testSet)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbf6b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "def loadDataSet():\n",
    "    from sklearn import datasets\n",
    "    iris_data = datasets.load_iris()\n",
    "    dataSets = pd.DataFrame(iris_data.data)\n",
    "    dataSets.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    dataSets['类别']=iris_data.target\n",
    "    return dataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbd2bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randSplit(dataSet,rate):\n",
    "    l = list(dataSet.index)\n",
    "    random.shuffle(l)\n",
    "    dataSet.index = l\n",
    "    m = dataSet.shape[0]\n",
    "    m_train = int(rate*m)\n",
    "    train_data = dataSet.loc[range(m_train),:]\n",
    "    test_data = dataSet.loc[range(m_train,m),:]\n",
    "    train_data.index = range(len(train_data))\n",
    "    test_data.index = range(len(test_data))\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a32242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnb_classify(train_data,test_data):\n",
    "    labels = list(set(train_data.iloc[:,-1]))\n",
    "    mean_list = []\n",
    "    var_list = []\n",
    "    for label in labels:\n",
    "        curr_label_data = train_data.loc[train_data.iloc[:,-1]==label,:]\n",
    "        m = curr_label_data.iloc[:,:-1].mean()\n",
    "        v = np.sum((curr_label_data.iloc[:,:-1]-m)**2)/curr_label_data.shape[0]\n",
    "        mean_list.append(m)\n",
    "        var_list.append(v)\n",
    "    mean_df = pd.DataFrame(mean_list,index=labels)\n",
    "    var_df = pd.DataFrame(var_list,index=labels)\n",
    "    result = []\n",
    "    for j in range(test_data.shape[0]):\n",
    "        curr_test = test_data.iloc[j,:-1].tolist()\n",
    "        predict_prob = np.exp(-1*(curr_test-mean_df)**2/(2*var_df))/(np.sqrt(2*np.pi*var_df))\n",
    "        prob = 1\n",
    "        for k in range(test_data.shape[1]-1):\n",
    "            prob *= predict_prob.iloc[:,k]\n",
    "        cla = prob.index[np.argmax(prob.values)]\n",
    "        result.append(cla)\n",
    "    test_data['predict']=result\n",
    "    acc = (test_data.iloc[:,-1]==test_data.iloc[:,-2]).mean()\n",
    "    print('预测准确率：',acc)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb8a0da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率： 0.8666666666666667\n",
      "预测准确率： 0.9666666666666667\n",
      "预测准确率： 0.9333333333333333\n",
      "预测准确率： 0.9666666666666667\n",
      "预测准确率： 0.9666666666666667\n",
      "预测准确率： 0.9\n",
      "预测准确率： 1.0\n",
      "预测准确率： 0.9333333333333333\n",
      "预测准确率： 0.8666666666666667\n",
      "预测准确率： 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "dataSet = loadDataSet()\n",
    "for i in range(10):\n",
    "    train_data, test_data = randSplit(dataSet, 0.8)\n",
    "    test_result = gnb_classify(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbn_classify_by_sklearn(train_data,test_data):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    gnb_clf = GaussianNB()\n",
    "    gnb_clf.fit(train_data.iloc[:,:-1],train_data.iloc[:,-1])\n",
    "    predict_class = gnb_clf.predict(test_data.iloc[:,-1])\n",
    "    test_data['predict']=list(predict_class)\n",
    "    acc = accuracy_score(test_data.iloc[:,-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
